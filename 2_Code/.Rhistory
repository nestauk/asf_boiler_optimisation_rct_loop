## The outcome is electricity consumption during a Demand Flexibility Service event.
## The data comprise electricity consumption the most recent event (Wh) for
## customers of Loop who have opted into the scheme overall.
## The code randomly allocates each customer to either a Treatment group or a
## Control group. It then uses the standard error of the Treatment estimate to
## determine the minimum detectable effect.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 0: Importing data ~~~ ###
# Import dummy data
data <- read_csv(paste(data_path, "EventData.csv", sep="/")) |>
janitor::clean_names()
#~~~ Cleaning and exploring
head(data)
str(data)
# Creating opt-out variable
data <- data |>
mutate( opt_in = case_when(
took_part == "TRUE" ~ 1,
took_part == "FALSE" ~ 0,
TRUE ~ 99
))
## Note, opting in to an event is our secondary outcome measure. We don't use
## this variable for the power calculations, but it is helpful to see.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 1: Exploring data ~~~ ###
# ~~ I. Usage ~~ #
#~~~ Summary statistics
summary(data$usage)
descr(data$usage)
nrow(data) ## 14,616 rows
length(data$usage[data$usage == 0]) ## 348 zeros (roughly 2%)
#~~~ Graphing
hist(data$usage, breaks = 1e3) ## Looks very skewed, with a lot of zeros
hist(log(data$usage[data$usage != 0]), breaks = 1e3) ## Looks more normal
hist(log(data$usage) +1, breaks = 1e3)
# ~~ II. Opt ins ~~ #
summary(data$opt_in)
tabyl(data$opt_in) # 61% opt-in
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 2: Power calculations ~~~ ###
## This is achieved using simulations
# ~~ I. Setting up loop ~~ #
#~~~ Parameters
sample_seq <- seq(1e3, 14e3, 500)
## NOTE TO REVIEWER - running all simulations takes
sims <- length(sample_seq) * 30 ## Number of simulations
##sims <- length(sample_seq) * 3 ## Number of simulations
sim <- 0
# Looping
for (i in 1:sims) {
# Sim count
sim <- sim + 1
#~~~ Selecting sample size
## We create a new dataframe so we don't need to import the data each time.
data_to_use <- data |>
filter(!duplicated(id)) |> ## Removing duplicates
mutate(rand = runif(1)) |> ## Generating random number
arrange(rand) ## Sorting by random number
# Subsetting
sample <- sample_seq[(sim %% (length(sample_seq)) +1)]
data_to_use <- data_to_use |>
filter(row_number() <= sample)
#~~~ Random treatment assignment
data_to_use <- data_to_use |>
group_by(id) |>
mutate(treat_num = sample(2, 1, replace = TRUE)) |>
ungroup() |>
mutate(treatment = case_when(
treat_num == 1 ~ "Control",
treat_num == 2 ~ "Treatment",
TRUE ~ "Missing"
))
# ~~ II. Running regression ~~ #
if (sim <= length(sample_seq) * 10) {
type <- "OLS"
# Fit model
fit <- lm(usage ~ treatment, data = data_to_use)
# Robust SEs
fit_coef <- coeftest(fit, vcov. = vcovHC, type = "HC3")
} else if (sim <= length(sample_seq) * 20) {
type <- "Logged"
data_to_use <- data_to_use |>
mutate(usage = log(usage + 1))
# Fit model
fit <- lm(usage ~ treatment, data = data_to_use)
# Robust SEs
fit_coef <- coeftest(fit, vcov. = vcovHC, type = "HC3")
} else {
type <- "Poisson"
# Fit model
fit <- glm(usage ~ treatment, family= "poisson", data = data_to_use)
# Robust SEs
fit_coef <- coeftest(fit, vcov. = vcovHC, type = "HC3")
}
# Converting to data frame
coef_df <- transpose(as.data.frame(fit_coef[2, ])) |>
mutate(beta = V1, se = V2, t_stat = V3, pval = V4) |>
select(-c(V1, V2, V3, V4))
# Control statistics
control_mean <- mean(data_to_use$usage[data_to_use$treatment == "Control"], na.rm = TRUE)
control_sd <- sd(data_to_use$usage[data_to_use$treatment == "Control"], na.rm = TRUE)
# ~~ IV. Aggregating ~~ #
# Binding columns
sim_data <- cbind(sim, sample, type, control_mean, control_sd, coef_df)
# Binding rows
if (sim == 1) {
output_data <- sim_data
# Exporting first regression to double check
sink(paste(out_dir,"first_regression_flex.txt", sep="/"))
# No covariates
fit_simple <- lm(usage ~ treatment, data = data_to_use)
print(summ(fit_simple))
print(summ(fit_simple, robust = "HC3"))
sink()
} else {
output_data <- rbind(output_data, sim_data)
}
}
#~~~ Export
write.csv(output_data, file = paste(out_dir,"flex_power.csv", sep = "/"))
# ~~ III. Summarising for specific numbers of Treatment arms ~~ #
#~~~ Setting up arms
output_data <- output_data |>
mutate(arms = case_when(
sample == 14000 ~ 2, ## 7,000 in each arm, so 14,000 for T/C comparison
sample == 9500 ~ 3, ## ~4,500 in each arm, so 9,000 for T/C comparison
sample == 7000 ~ 4, ## ~3,500 in each arm, so 7,000 for T/C comparison
sample == 5500 ~ 5, ##  ~2,8000 in each arm, so ~5,500 for T/C comparison
TRUE ~ 99
)
)
# Summarising
output_table <- output_data |>
group_by(type, sample) |>
summarise(
control_mean = mean(control_mean, na.rm = TRUE),
control_sd = mean(control_mean, na.rm = TRUE),
beta = mean(beta, na.rm = TRUE),
se = mean(se, na.rm = TRUE),
arms = mean(arms, na.rm = TRUE)
)
# Filtering for experimental arms
output_table <- output_table |>
filter(arms != 99)
# Adjusting alpha for multiple comparisons (Bonferroni)
output_table$alpha_adj <-  0.05 / (output_table$arms - 1)
## Rule of 2.8, two-tailed
output_table$mde <-  output_table$se * (qnorm(0.8) + qnorm (1 - output_table$alpha_adj /2 ))
## Calculating % change
output_table$mde_prop <- 0
output_table$mde_prop[output_table$type == "OLS"] <- output_table$mde[output_table$type == "OLS"] / output_table$control_mean[output_table$type == "OLS"]
output_table$mde_prop[output_table$type != "OLS"] <- output_table$mde[output_table$type != "OLS"]
#~~~ Export
write.csv(output_table, file = paste(out_dir,"flex_power_arms.csv", sep = "/"))
end_time <- Sys.time()
end_time - start_time
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ END ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Load packages
library(tidyverse) #----- Manipulating data (`case_when`)
library(janitor) #------- Useful for cleaning
library(stringr) #------- String operations (`str_wrap`)
library(data.table) #---- For transposing
library(readxl) #-------- Reading Excel files
library(blockrand) #----- Stratified randomisation
# Data pathways
bas_dir <- "C:/Users/oli.berry/Documents/GitHub/asf-boiler-optimisation-rct-loop"
data_path <- file.path(bas_dir, "1_Data")
out_dir <- file.path(bas_dir, "3_Outputs")
# Set seed
set.seed(20230103)
# Functions
export_table <- function(dat, varname1, varname2){
dat |>
tabyl(.data[[varname1]], .data[[varname2]], show_na = FALSE) |>
adorn_totals(c("row", "col")) |>
adorn_percentages("col") |>
adorn_pct_formatting(digits = 0) |>
adorn_ns() |>
as.data.frame() |>
write.csv(paste(paste(out_dir,paste(varname1, varname2, sep="_"), sep="/"), "csv", sep="."))
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Summary of code ~~~ ###
## This code is used to randomise Loop's customers into one of three experimental
## arms: Treatment 1 (T1), Treatment 2 (T2), or Control group (C)
## We used stratified randomisation to achieve this, stratifying on region. This
## is because region correlates with gas consumption because of weather and other
## regional factors.
## Additionally, we allocate customers in Treatment 1 or Treatment 2 to one of
## four email variants, as follows:
## 1) Standard email
## 2) Thermostat advice variant
## 3) TRV advice variant
## 4) Social norms variant
## This will be achieved using simple randomisation.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 0: Importing data ~~~ ###
# Description of dataset required for code
# We need one observation for each customer and their individual-level covariates
# (as outlined below):
# user_id ~ unique customer identifier
# region_id ~ region identified for each customer (as defined by Nomenclature of Territorial Units for Statistics 1)
# time_with_loop ~ Length of time the participant has used Loop (categorical as follows: (a) 1 month or less; (b) 1 to 3 months; (c) 3 to 6 months); and (d) 6 or more months)
# proportion_of_emails_read ~ % of opening rates of up to 5 previous Loop emails (categorical, as follows: (a) Participant has opened each of the emails; (b) Participant has opened some of the emails; (c) Participants has not opened any of the emails)
# annual_consumption_band ~ Annual energy consumption bands (categorical, 500 kWh buckets)
# Import dummy data
data <- read_excel(paste(data_path, "DummyAllocation.xlsx", sep="/")) |>
janitor::clean_names()
# Load packages
library(tidyverse) #----- Manipulating data (`case_when`)
library(janitor) #------- Useful for cleaning
library(stringr) #------- String operations (`str_wrap`)
library(data.table) #---- For transposing
library(readxl) #-------- Reading Excel files
library(blockrand) #----- Stratified randomisation
# Data pathways
bas_dir <- "C:/Users/oli.berry/Documents/GitHub/Loop-BIT-boiler"
data_path <- file.path(bas_dir, "1_Data")
out_dir <- file.path(bas_dir, "3_Outputs/5_Randomisation")
# Set seed
set.seed(20230103)
# Functions
export_table <- function(dat, varname1, varname2){
dat |>
tabyl(.data[[varname1]], .data[[varname2]], show_na = FALSE) |>
adorn_totals(c("row", "col")) |>
adorn_percentages("col") |>
adorn_pct_formatting(digits = 0) |>
adorn_ns() |>
as.data.frame() |>
write.csv(paste(paste(out_dir,paste(varname1, varname2, sep="_"), sep="/"), "csv", sep="."))
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Summary of code ~~~ ###
## This code is used to randomise Loop's customers into one of three experimental
## arms: Treatment 1 (T1), Treatment 2 (T2), or Control group (C)
## We used stratified randomisation to achieve this, stratifying on region. This
## is because region correlates with gas consumption because of weather and other
## regional factors.
## Additionally, we allocate customers in Treatment 1 or Treatment 2 to one of
## four email variants, as follows:
## 1) Standard email
## 2) Thermostat advice variant
## 3) TRV advice variant
## 4) Social norms variant
## This will be achieved using simple randomisation.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 0: Importing data ~~~ ###
# Description of dataset required for code
# We need one observation for each customer and their individual-level covariates
# (as outlined below):
# user_id ~ unique customer identifier
# region_id ~ region identified for each customer (as defined by Nomenclature of Territorial Units for Statistics 1)
# time_with_loop ~ Length of time the participant has used Loop (categorical as follows: (a) 1 month or less; (b) 1 to 3 months; (c) 3 to 6 months); and (d) 6 or more months)
# proportion_of_emails_read ~ % of opening rates of up to 5 previous Loop emails (categorical, as follows: (a) Participant has opened each of the emails; (b) Participant has opened some of the emails; (c) Participants has not opened any of the emails)
# annual_consumption_band ~ Annual energy consumption bands (categorical, 500 kWh buckets)
# Import dummy data
data <- read_excel(paste(data_path, "DummyAllocation.xlsx", sep="/")) |>
janitor::clean_names()
## This can be ignored when using the correct dataset.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 1: Cleaning & merging ~~~ ###
# Cleaning
data <- data |>
mutate(across(c("user_id", "region_id", "time_with_loop", "proportion_of_emails_read", "annual_consumption_band"), as.factor))
# Creating band for annual consumption
## These are based on pre-established bands
data <- data |>
mutate(annual_consumption = as.integer(annual_consumption_band) * 500) |> ## Each band in 500 kWh
mutate(annual_band = case_when(
annual_consumption < 8000 ~ 1,
annual_consumption < 12000 ~ 2,
annual_consumption < 17000 ~ 3,
annual_consumption >= 17000 ~ 4,
TRUE ~ 99
))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 2: Randomising ~~~ ###
# ~~ I. Generating random number ~~ #
## We generate a random number for each individual to randomise the order. This
## is performed so the stratified randomisation can be double-checked and also
## facilitates a random start to allocation for each region.
data <- data |>
filter(!duplicated(user_id)) |> ## Removing duplicates
mutate(allocation = runif(1)) |> ## Generating random number
arrange(allocation) |> ## Sorting by random number
group_by(region_id) |>
mutate(region_ran = sample(3, 1, replace = TRUE)) |> ## Creating random number for each region
ungroup()
View(data)
# Creating treatment assignment
data <- data |>
group_by(region_id) |>
mutate(treatment_num = ((row_number() + floor(region_ran)) %% 3)) |>
mutate(treatment = case_when(
treatment_num == 0 ~ "T1",
treatment_num == 1 ~ "T2",
treatment_num == 2 ~ "C",
TRUE ~ "Missing"
)) |>
ungroup()
data <- data |>
filter(!duplicated(user_id)) |> ## Removing duplicates
mutate(allocation = runif(1)) |> ## Generating random number
arrange(allocation) |> ## Sorting by random number
group_by(region_id) |>
mutate(region_ran = sample(3, 1, replace = TRUE)) |> ## Creating random number for each region
ungroup()
# ~~ II. Stratifying ~~ #
## We allocate each row in sequence to either T1, T2, or C, starting on each
## randomly due to the random region_num
# Creating treatment assignment
data <- data |>
group_by(region_id) |>
mutate(treatment_num = ((row_number() + floor(region_ran)) %% 3)) |>
mutate(treatment = case_when(
treatment_num == 0 ~ "T1",
treatment_num == 1 ~ "T2",
treatment_num == 2 ~ "C",
TRUE ~ "Missing"
)) |>
ungroup()
# Load packages
library(tidyverse) #----- Manipulating data (`case_when`)
library(janitor) #------- Useful for cleaning
library(stringr) #------- String operations (`str_wrap`)
library(data.table) #---- For transposing
library(readxl) #-------- Reading Excel files
library(blockrand) #----- Stratified randomisation
# Data pathways
bas_dir <- "C:/Users/oli.berry/Documents/GitHub/Loop-BIT-boiler"
data_path <- file.path(bas_dir, "1_Data")
out_dir <- file.path(bas_dir, "3_Outputs/5_Randomisation")
# Set seed
set.seed(20230103)
# Functions
export_table <- function(dat, varname1, varname2){
dat |>
tabyl(.data[[varname1]], .data[[varname2]], show_na = FALSE) |>
adorn_totals(c("row", "col")) |>
adorn_percentages("col") |>
adorn_pct_formatting(digits = 0) |>
adorn_ns() |>
as.data.frame() |>
write.csv(paste(paste(out_dir,paste(varname1, varname2, sep="_"), sep="/"), "csv", sep="."))
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Summary of code ~~~ ###
## This code is used to randomise Loop's customers into one of three experimental
## arms: Treatment 1 (T1), Treatment 2 (T2), or Control group (C)
## We used stratified randomisation to achieve this, stratifying on region. This
## is because region correlates with gas consumption because of weather and other
## regional factors.
## Additionally, we allocate customers in Treatment 1 or Treatment 2 to one of
## four email variants, as follows:
## 1) Standard email
## 2) Thermostat advice variant
## 3) TRV advice variant
## 4) Social norms variant
## This will be achieved using simple randomisation.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 0: Importing data ~~~ ###
# Description of dataset required for code
# We need one observation for each customer and their individual-level covariates
# (as outlined below):
# user_id ~ unique customer identifier
# region_id ~ region identified for each customer (as defined by Nomenclature of Territorial Units for Statistics 1)
# time_with_loop ~ Length of time the participant has used Loop (categorical as follows: (a) 1 month or less; (b) 1 to 3 months; (c) 3 to 6 months); and (d) 6 or more months)
# proportion_of_emails_read ~ % of opening rates of up to 5 previous Loop emails (categorical, as follows: (a) Participant has opened each of the emails; (b) Participant has opened some of the emails; (c) Participants has not opened any of the emails)
# annual_consumption_band ~ Annual energy consumption bands (categorical, 500 kWh buckets)
# Import dummy data
data <- read_excel(paste(data_path, "DummyAllocation.xlsx", sep="/")) |>
janitor::clean_names()
## This can be ignored when using the correct dataset.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 1: Cleaning & merging ~~~ ###
# Cleaning
data <- data |>
mutate(across(c("user_id", "region_id", "time_with_loop", "proportion_of_emails_read", "annual_consumption_band"), as.factor))
# Creating band for annual consumption
## These are based on pre-established bands
data <- data |>
mutate(annual_consumption = as.integer(annual_consumption_band) * 500) |> ## Each band in 500 kWh
mutate(annual_band = case_when(
annual_consumption < 8000 ~ 1,
annual_consumption < 12000 ~ 2,
annual_consumption < 17000 ~ 3,
annual_consumption >= 17000 ~ 4,
TRUE ~ 99
))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 2: Randomising ~~~ ###
# ~~ I. Generating random number ~~ #
## We generate a random number for each individual to randomise the order. This
## is performed so the stratified randomisation can be double-checked and also
## facilitates a random start to allocation for each region.
data <- data |>
filter(!duplicated(user_id)) |> ## Removing duplicates
mutate(allocation = runif(1)) |> ## Generating random number
arrange(allocation) |> ## Sorting by random number
group_by(region_id) |>
mutate(region_ran = sample(3, 1, replace = TRUE)) |> ## Creating random number for each region
ungroup()
# ~~ II. Stratifying ~~ #
## We allocate each row in sequence to either T1, T2, or C, starting on each
## randomly due to the random region_num
# Creating treatment assignment
data <- data |>
group_by(region_id) |>
mutate(treatment_num = ((row_number() + floor(region_ran)) %% 3)) |>
mutate(treatment = case_when(
treatment_num == 0 ~ "T1",
treatment_num == 1 ~ "T2",
treatment_num == 2 ~ "C",
TRUE ~ "Missing"
)) |>
ungroup()
# Load packages
library(tidyverse) #----- Manipulating data (`case_when`)
library(janitor) #------- Useful for cleaning
library(stringr) #------- String operations (`str_wrap`)
library(data.table) #---- For transposing
library(readxl) #-------- Reading Excel files
library(blockrand) #----- Stratified randomisation
# Data pathways
bas_dir <- "C:/Users/oli.berry/Documents/GitHub/Loop-BIT-boiler"
data_path <- file.path(bas_dir, "1_Data")
out_dir <- file.path(bas_dir, "3_Outputs/5_Randomisation")
# Set seed
set.seed(2023003)
# Functions
export_table <- function(dat, varname1, varname2){
dat |>
tabyl(.data[[varname1]], .data[[varname2]], show_na = FALSE) |>
adorn_totals(c("row", "col")) |>
adorn_percentages("col") |>
adorn_pct_formatting(digits = 0) |>
adorn_ns() |>
as.data.frame() |>
write.csv(paste(paste(out_dir,paste(varname1, varname2, sep="_"), sep="/"), "csv", sep="."))
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Summary of code ~~~ ###
## This code is used to randomise Loop's customers into one of three experimental
## arms: Treatment 1 (T1), Treatment 2 (T2), or Control group (C)
## We used stratified randomisation to achieve this, stratifying on region. This
## is because region correlates with gas consumption because of weather and other
## regional factors.
## Additionally, we allocate customers in Treatment 1 or Treatment 2 to one of
## four email variants, as follows:
## 1) Standard email
## 2) Thermostat advice variant
## 3) TRV advice variant
## 4) Social norms variant
## This will be achieved using simple randomisation.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 0: Importing data ~~~ ###
# Description of dataset required for code
# We need one observation for each customer and their individual-level covariates
# (as outlined below):
# user_id ~ unique customer identifier
# region_id ~ region identified for each customer (as defined by Nomenclature of Territorial Units for Statistics 1)
# time_with_loop ~ Length of time the participant has used Loop (categorical as follows: (a) 1 month or less; (b) 1 to 3 months; (c) 3 to 6 months); and (d) 6 or more months)
# proportion_of_emails_read ~ % of opening rates of up to 5 previous Loop emails (categorical, as follows: (a) Participant has opened each of the emails; (b) Participant has opened some of the emails; (c) Participants has not opened any of the emails)
# annual_consumption_band ~ Annual energy consumption bands (categorical, 500 kWh buckets)
# Import dummy data
data <- read_excel(paste(data_path, "DummyAllocation.xlsx", sep="/")) |>
janitor::clean_names()
## This can be ignored when using the correct dataset.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 1: Cleaning & merging ~~~ ###
# Cleaning
data <- data |>
mutate(across(c("user_id", "region_id", "time_with_loop", "proportion_of_emails_read", "annual_consumption_band"), as.factor))
# Creating band for annual consumption
## These are based on pre-established bands
data <- data |>
mutate(annual_consumption = as.integer(annual_consumption_band) * 500) |> ## Each band in 500 kWh
mutate(annual_band = case_when(
annual_consumption < 8000 ~ 1,
annual_consumption < 12000 ~ 2,
annual_consumption < 17000 ~ 3,
annual_consumption >= 17000 ~ 4,
TRUE ~ 99
))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
### ~~~ Part 2: Randomising ~~~ ###
# ~~ I. Generating random number ~~ #
## We generate a random number for each individual to randomise the order. This
## is performed so the stratified randomisation can be double-checked and also
## facilitates a random start to allocation for each region.
data <- data |>
filter(!duplicated(user_id)) |> ## Removing duplicates
mutate(allocation = runif(1)) |> ## Generating random number
arrange(allocation) |> ## Sorting by random number
group_by(region_id) |>
mutate(region_ran = sample(3, 1, replace = TRUE)) |> ## Creating random number for each region
ungroup()
# ~~ II. Stratifying ~~ #
## We allocate each row in sequence to either T1, T2, or C, starting on each
## randomly due to the random region_num
# Creating treatment assignment
data <- data |>
group_by(region_id) |>
mutate(treatment_num = ((row_number() + floor(region_ran)) %% 3)) |>
mutate(treatment = case_when(
treatment_num == 0 ~ "T1",
treatment_num == 1 ~ "T2",
treatment_num == 2 ~ "C",
TRUE ~ "Missing"
)) |>
ungroup()
